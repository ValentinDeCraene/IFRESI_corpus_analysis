import os
import spacy
import pandas as pd
from collections import Counter
from tqdm import tqdm
from typing import Dict, Tuple, List

# ---------------------------- CONFIGURATION --------------------------------
SPACY_MODEL = "fr_core_news_sm"  # Mod√®le linguistique pour le traitement en fran√ßais
DATA_DIR = "data/"
OUTPUT_FILE = "data_analysis/entities_with_types_and_count.csv"


# ---------------------------- CHARGEMENT DU MOD√àLE SPACY --------------------
def load_spacy_model(model_name: str) -> spacy.language.Language:
    """Charge le mod√®le SpaCy et g√®re les erreurs potentielles."""
    try:
        nlp = spacy.load(model_name)
        nlp.max_length = 2_000_000  # Augmentation de la limite pour √©viter les erreurs
        return nlp
    except Exception as e:
        raise RuntimeError(f"Erreur lors du chargement du mod√®le SpaCy : {e}")


nlp = load_spacy_model(SPACY_MODEL)


# ---------------------------- CHARGEMENT DES TEXTES ------------------------
def load_text_files(directory: str) -> Dict[str, str]:
    """Charge les fichiers .txt d'un r√©pertoire et ses sous-dossiers, en excluant les logs."""
    if not os.path.exists(directory):
        raise FileNotFoundError(f"Le dossier '{directory}' n'existe pas. V√©rifiez le chemin.")

    text_data = {}
    files_found = 0

    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".txt") and "log" not in file.lower():  # Exclure les fichiers logs
                file_path = os.path.join(root, file)
                with open(file_path, "r", encoding="utf-8") as f:
                    text_data[file_path] = f.read()
                    files_found += 1

    print(f"‚úÖ Nombre de fichiers trouv√©s (hors logs) : {files_found}")
    return text_data


# ---------------------------- EXTRACTION DES ENTIT√âS NOMM√âES --------------
def extract_named_entities(corpus: Dict[str, str]) -> Counter:
    """Extrait les entit√©s nomm√©es du corpus et les compte."""
    entity_data = Counter()

    for filename, text in tqdm(corpus.items(), desc="Extraction des entit√©s nomm√©es", unit="doc"):
        doc = nlp(text)
        for ent in doc.ents:
            # Utilisation d'une cl√© combin√©e pour √©viter les doublons
            entity_data[(ent.text, ent.label_)] += 1

    return entity_data


# ---------------------------- ENREGISTREMENT DES R√âSULTATS ----------------
def save_results(entity_data: Counter, output_file: str):
    """Enregistre les entit√©s extraites et leur fr√©quence dans un fichier CSV."""
    # Classement des entit√©s par fr√©quence
    sorted_entities = entity_data.most_common()

    # Pr√©paration des donn√©es pour le DataFrame
    data_for_df = [(entity[0][0], entity[0][1], entity[1]) for entity in sorted_entities]

    # Enregistrement dans un fichier CSV
    df = pd.DataFrame(data_for_df, columns=["Entit√©", "Type d'entit√©", "Fr√©quence"])
    df.to_csv(output_file, index=False, encoding="utf-8")
    print(f"\nüìÇ R√©sultats enregistr√©s dans '{output_file}'.")


# ---------------------------- EX√âCUTION PRINCIPALE ------------------------
def main():
    """Fonction principale pour charger les donn√©es, extraire les entit√©s et enregistrer les r√©sultats."""
    # Chargement du corpus
    corpus = load_text_files(DATA_DIR)
    print(f"üìÇ Nombre de documents charg√©s : {len(corpus)}")

    # Extraction des entit√©s nomm√©es
    entities = extract_named_entities(corpus)

    # Enregistrement des r√©sultats
    save_results(entities, OUTPUT_FILE)

    # Affichage des 10 entit√©s les plus fr√©quentes
    print("\nüîç 10 entit√©s nomm√©es les plus fr√©quentes avec leur type:")
    for entity in list(entities.most_common())[:10]:
        print(f"{entity[0][0]} ({entity[0][1]}): {entity[1]} fois")


# ---------------------------- EX√âCUTION DU SCRIPT -------------------------
if __name__ == "__main__":
    main()
